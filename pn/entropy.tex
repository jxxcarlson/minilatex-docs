beginMetadata:
{
    "id": "eda86b76-c70e-419c-ab4d-002d4bd716ef",
    "documentNumber": 8,
    "author": "jxxcarlson",
    "title": "Entropy",
    "path": "pn/entropy.tex",
    "tags": [],
    "keyString": "jxxcarlson pn/entropy.tex",
    "timeCreated": 1598451767654,
    "timeModified": 1598451774700,
    "public": true,
    "collaborators": [],
    "docType": "miniLaTeX",
    "versionNumber": 2,
    "versionDate": 1598451803655
}
endMetadata
\xlink{uuid:8df12037-c8a4-4e22-be40-4939e94bd9e9}{Physics Notes}

\setcounter{section}{2}

\section{Entropy}

The entropy of a system that has $n$ states, each of which occurs with probability $p_k$ is defined to be

\begin{equation}
\label{eq:entropy}
S(p_1, p_2, \ldots p_n) = - \sum_k p_k \log p_k
\end{equation}

If the probabilities are equal, then $S = \log n$.  This is the maximum entropy, where the maximum is taken over all probability distributiions $(p_1, \ldots , p_k)$.  On the other hand, if $p_k  = 1$ and all the other probabilities are zero, then $S = 0$.  Thus $S$ has the right qualitative properties: it is small for a highly ordered system in which only one state occurs, and it is large for a highly disorded system in which all states occur with equal probability.

Shannon gave an argument which shows that \eqref{eq:entropy} is the
only formula, up to multiplicative constant, that satisfies certain simple properties:

\begin{enumerate}


\item $S(1/p_1, \ldots 1/p_n)$  is continuous.

\item $S(1/n, \ldots 1/n) $ is maximal for all $S(1/p_1, \ldots 1/p_n)$ .

\item $S$ is additive in the sense that
$S(AB) = S(A) + \sum p_kS(B|A)$.

\end{enumerate}

The additivity condition can be motivated as follows. One has a set of $m$ buckets where the $k$-th bucket contains $n_k$ objects.  Experiment $A$ is to select a bucket at random.  The $k$-th bucket is selected with probability $p_k$.  Experiment $B$ is to select an object from the $k$-th bucket. The entropy ("uncertainty") of the experiment is $S(AB)$.  The entropy of the first step is $S(A)$.  The expected value of the entropy of the second step is $p_1S(B|A) + \cdots p_mS(B|A)$.

Shannnon proceeds as follows.  Let $f(N) = S(1/N, \ldots 1/N)$.  Suppose that bucket $k$ is selected with probability $p_k = n_k/N$.  Then the additivity relation reads

\begin{equation}
f(N) = S(p_1, \ldots, p_m) + \sum_{k=1}^m p_k f(n_k)
\end{equation}


Now suppose that the buckets contain equal numbers of objects, so that $n_k = n$, where $N = mn$.  Since $p_k = n_k/N = 1/m$, we have

\begin{equation}
f(N) = S(1/m, \ldots, 1/m) + \sum_{k=1}^m \frac{1}{m} f(n),
\end{equation}

or simply


\begin{equation}
f(mn) = f(m) + f(n).
\end{equation}


The only continuous solution to this functional equation is $f(m) = K\log m$
for some constant $K$.  Thus we have

\begin{equation}
K \log n  = S(p_1, \ldots, p_m) + \sum p_k K \log n_k
\end{equation}

so that

\begin{align}
  S(p_1, \ldots, p_m)            & = K(\log N - \sum p_k K \log n_k) \\
             & = K(\sum p_k \log N - \sum p_k K \log n_k) \\
             & = K \sum p_k \log \frac{N}{n_k} \\
             & = - K \sum p_k \log p_k
\end{align}

\subheading{References}

 \href{http://micro.stanford.edu/~caiwei/me334/Chap7_Entropy_v04.pdf}{Introduction to Statistical Mechanics, Stanford Univeristy, Handout for chapter 7}

\href{https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4}{Shannon Entropy, Information Gain, and Picking Balls from Buckets}
